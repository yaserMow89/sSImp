CNI
===
- Different **Bridge** networking options
   - All combined into a single program called `bridge`
      - All the things required to attach a container to a bridge network
      - For example you can add a container to a particular specific `namespace`
         - `bridge add <contID> /var/run/netns/<contID>`
      - Another example would be when `rkt` or `k8s` create a new container, they call the `bridge` program and pass the container id and namespace to get networking configured for that container
         - `bridge add <cid> <namespace>`
- What if anyone wants to creates their own one, what should they follow to get something compatible with the whole stack
- That is where *`CNI`* (**Container Network Interface**)
- A set of standards define how programs should be developed to solve networking challenges, in a container runtime environment
   - The programs are referred to as **plugins**
   - In our case the `bridge` program is a **plugin** for **CNI**
   - **CNI** defines how the plugin should be developed, and how container runtime should invoke them
   - Defines a set of **responsibilities** for *container runtimes* and *plugins*
      - Following the rules all *container runtimes* should be able to work with all *plugins*
      - CNI comes with a set of already available plugins such as `bridge`, `vlan`, `ipvlan`, `macvlan`, `windows` and etc..
   - But **Docker** does not implement **CNI**
      - Has got it's own set of standards, known as **CNM** (*Container Network Model*)
   - When K8S creates docker containers, it creates them with `--network=none` and then invokes the configured CNI plugins to take care of the rest of configuration
      - Since the docker does not implement **CNI**
